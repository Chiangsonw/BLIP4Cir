import torch
from torch import nn
import torch.nn.functional as F
import sys
from utils import device

class Twin_Attention_Compositor(nn.Module):
    def __init__(self) -> None:
        super().__init__()
        self.fusion = nn.Linear(512,256)
        self.relu1 = nn.ReLU(inplace=True)
        self.MHAttention_1 = torch.nn.MultiheadAttention(embed_dim=256, num_heads=1, dropout=0.0, batch_first=True)
        self.MHAttention_2 = torch.nn.MultiheadAttention(embed_dim=256, num_heads=1, dropout=0.0, batch_first=True)

    def forward(self, reference_embeddings:torch.tensor, target_embeddings:torch.tensor):
        bs, length, hidden = reference_embeddings.size()
        #embeddings to tokens  bs x length x hidden    bs 32 256

        #4 layers of attention
        output1, attention_weights1 = self.MHAttention_1(query=reference_embeddings, key=target_embeddings, value=target_embeddings)
        output1, attention_weights1 = self.MHAttention_1(query=reference_embeddings, key=output1, value=output1)
        output1, attention_weights1 = self.MHAttention_1(query=reference_embeddings, key=output1, value=output1)
        output1, attention_weights1 = self.MHAttention_1(query=reference_embeddings, key=output1, value=output1)

        #4 layers of attention
        output2, attention_weights2 = self.MHAttention_2(query=target_embeddings, key=reference_embeddings, value=reference_embeddings)
        output2, attention_weights2 = self.MHAttention_2(query=target_embeddings, key=output2, value=output2)
        output2, attention_weights2 = self.MHAttention_2(query=target_embeddings, key=output2, value=output2)
        output2, attention_weights2 = self.MHAttention_2(query=target_embeddings, key=output2, value=output2)

        #use cls token 作为 features bs x 256  两个features平均
        output1_features = output1[:,0,:]
        output2_features = output2[:,0,:]
        output_features = (output1_features + output2_features)/2
        #output_features = self.relu1(self.fusion(torch.concat([output1_features,output2_features]),dim=-1))
        return output_features
    


# reference_tensor = torch.rand(128, 2560, 9, 9).to(device, non_blocking=True)
# target_tensor = torch.rand(128, 2560, 9, 9).to(device, non_blocking=True)

# discrepancy_m = Discrepancy_transformer().to(device, non_blocking=True)
# diff_features = discrepancy_m(reference_tensor, target_tensor)

# print(diff_features.size())


